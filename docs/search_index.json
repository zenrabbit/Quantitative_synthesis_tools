[["index.html", "Chapter 1 Welcome to a short synthesis course", " Chapter 1 Welcome to a short synthesis course An overview of quantitative synthesis tools associated with review processing. Learning outcomes Critically review peer-reviewed journal publications. Engineer syntheses and solutions from published evidence. Appreciate strengths of different synthesis tools and reporting. Do a meta-analysis or systematic review. Rationale Scientific synthesis of all academic evidence from data to papers can promote evidence-informed decision making. Every contemporary discipline has some capacity to support and benefit from synthesis tools. Here, we develop a set of heuristics to support innovation and leverage evidence. Structure A overview of lessons provided in this resource to encourage development and use of synthesis tools relevant to your discipline and challenges. lesson topic goal key resource 1 Quantitative synthesis tools Explore conceptual and mental models for synthesis. The power of data synthesis to shape the future of the restoration community and capacity 2 Evidence workflow reporting Develop reporting documentation from a formal synthesis review process. Systematic review and meta-analysis: a primer 3 Meta-analysis in R Develop a template for derived data for analyses, and complete a meta-analysis in R. The metafor Package: A Meta-Analysis Package for R 4 Interpretation of meta-analyses Examine model outputs from meta-analyses and interpret. Identify key components. Meta-evaluation of meta-analysis: ten appraisal questions for biologists License This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["tools.html", "Chapter 2 Quantitative synthesis tools", " Chapter 2 Quantitative synthesis tools Learning outcomes Hone your capacity to review scientific literature and scope critical ideas. Transform your research expertise into formal scientific evidence. Synthesize peer-reviewed science papers. Context To best explore conceptual and mental models for synthesis, a summary of the simple landscape of opportunities and more comprehensive mapping of synthesis and critical big-picture thinking in science is illuminating. Scientific synthesis is big picture science that describe a set of studies on a pinpointed topic (Lortie 2014). There are at least three simple and direct options that are amenable to a capture of the research associated with a scientific challenge that you have identified. Narrative review as a highlight, short commentary, or new idea paper that is a snapshot of the key findings from a field research summarizing the main discoveries and/or listing the most critical research gaps. Papers like these are often called Insights, Forum, or Ignite. Systematic reviews are similar to narrative reviews, but clear criteria are listed explaining how you selected papers, i.e. these search terms were used in the Web of Science and only studies that had these key inclusion attributes were used. Systematic are more replicable because others can follow your steps and get the same set of papers and hopefully reach similar conclusions about the corpus of evidence. These reviews also typically provide some simple quantitative data about the set of studies such as number of countries where the research was done, total sample sizes, number of variables examined, or any attributes that describe what the research was for a specific detail. The narrative component is similar to the first option because it can state what we know and what do not but these reviews do so much more precisely. Even a few numbers go a long way to convincing people about the extent that we know or have studied a subject in science. Papers like these are often termed Short Commentary or Mini-Review. Meta-analyses are systematic reviews plus for each primary study you summarize, you capture the relative efficacy of the treatment tested. Papers like this are often termed Reviews or Meta-analyses but other terms can be used too. Note: in some fields of research the terms ‘systematic review’ and ‘meta-analysis’ are used interchangeably, but in most environmental sciences, meta-analyses always have a measure of the strength of evidence from each studied included in the synthesis whilst systematic reviews typically do not. So for instance, narrative review might provide insights into vaccine research and report that we have tested three vaccine types but need to test more alternatives. A systematic review would state this too but mention how they checked the science, i.e. we checked 100 papers using these terms x,y, &amp; z in The Web of Science, and it might also state how many people were tested in total across all studies. This is a more powerful synthesis. However, the gold standard would be the meta-analysis that summarizes all of the above but also reports how well each vaccine type tested actually works on average across all the studies. Summary of options synthesis elements benefits limitations narrative review summary, insights, next steps more opinion, can be shorter, less detailed processing the research literature can be less compelling without some specific evidence listed and is difficult to repeat systematic review summary, insights, next steps, explanation of how studies were selected in the review, can have counts of ideas tested more specific, can be repeated, and can be more compelling need to keep track of how you selected papers, need to sort through papers in more detail in addition to capturing main ideas meta-analyses all of above possible but must also include an assessment of the strength of evidence of each study included in the synthesis gold standard, reader can get a sense of how effective a treatment or intervention is relative to another need to extract means or measures of efficacy with sample sizes or variance from each study Evidence framing A more complex and comprehensive summary of the contemporary landscape of synthesis has been evolving. Framing scientific synthesis activities as an open, comprehensive, and diverse aggregation or summary of many forms of evidence expands our capacity to inform decision making. Contemporary synthesis science has innovated on evidence maps as a tool to show the evidence. This can be done through mapping evidence onto a geographic map to show where we know what we know (McKinnon et al. 2015). Evidence maps can also be less literal and map out knowledge as tabular or bubble plots that focus more on the relative frequencies of what we know about different concepts within a synthesis (Miake-Ley et al. 2016). This approach has also been framed as ‘evidence maps for evidence gaps’ because it can become abundantly clear where evidence is missing in terms of research or ideas (O’Leary et al. 2017). Importantly, decision making has also evolved to more substantially rely on synthesis science to inform strategy (Thomas-Walters et al. 2021). There has also been a drive to thus enable and support more synthesis to build capacity and connect disparate fields of research for more holistic solution sets (Ladouceur and Shackelford 2021). Challenge Review the literature from your field of research from the perspective of synthesis. Explore whether a simple three-paper typology is relevant or if the field has begun to incorporate and frame knowledge via other inference tools including these or even others. Methods, concepts, semantics, ontologies, etc. any forms of evidence aggregation that advances insight, innovation, and knowledge production. Familiarize yourself with the scientific synthesis options that can describe and capture the state-of-the-art research for your specific challenge. Select and refine your topic by populating a table with key terms. Check Google Scholar and The Web of Science trying these different terms. Document the relevant frequency of synthesis studies using these terms. Filter to most cited or the last three years of research only if the evidence is too extensive or if you suspect there is a key temporal bias or change within the field of inquiry. Repeat process for primary studies. Contrast the primary study focus with a synthesis focus. Do a cursory read of the abstracts of the synthesis studies. Identify other terms including synonyms and antonyms that you may have not included. Reflect on this challenge and ensure the terms you used and the papers you have are studying the dimension of the challenge you want to summarize. Decide if it makes sense to do a short narrative, systematic review, or meta-analysis (at least at this point in time). Products A clear vision of the challenge you want to tackle. A set of ideas, papers, and the outcome that you ultimately are likely to provide. The landscape of evidence you need to examine a process or challenge across many studies. Resources Slide deck Template to contrast relative frequencies of synthesis versus primary studies Reflection questions What synthesis studies have most informed or inspired your understanding in your field? Is there evidence that is most needed by stakeholders right now that needs to be compiled or derived? Was there any indication that the focus of the synthesis studies you retrieved differed from the primary studies? Do synthesis studies advance theory or depth of knowledge more rapidly than well-executed primary studies or experiments? "],["reporting.html", "Chapter 3 Quantitative synthesis workflow reporting", " Chapter 3 Quantitative synthesis workflow reporting Learning outcomes Develop a checklist for reading systematic reviews and meta-analyses. Document a quantitative synthesis process. Consolidate understanding of key synthesis elements from the literature. Context Evidence implementation and reuse is a non-trivial process in most disciplines. It is crucial that experts are able to use, reuse, and implement synthesis findings. This process of critical appraisal furthers a novel, big-picture view of scientific findings associated with single studies. This can take the form of a perspective that routinely weights relative evidence by purpose, reason, and other findings to improve decision making by stakeholders, the public, and other scientists. Application of this process to published peer-review evidence can be limited by transparency, level of reporting, missing data, meta-data articulation, and limited moderator reporting. Hence, ten simple rules for evidence reuse were proposed were broadly to highlight these and other challenges in synthesis science (Lortie and Owen 2020). Critical reading of meta-analyses is also a powerful skill for all experts (Lortie et al. 2013). This thinking and evaluation process has also been developed into a more prescriptive set of ten questions to apply to any published meta-analysis or systematic review (Nakagawa et al. 2017). These questions strengthen the reuse of current meta-analyses and provide a checklist for reporting in future systematic reviews and meta-analyses. Checklist reporting List from (Nakagawa et al. 2017): Is the search systematic and transparently documented? What question and what effect size? Is non-independence taken into account? Which meta-analytic model? Is the level of consistency among studies reported? Are the causes of variation among studies investigated? Are effects interpreted in terms of biological importance? Has publication bias been considered? Are results really robust and unbiased? Is the current state (and lack) of knowledge summarized? Purpose Good reporting is supported by good thinking. Purpose prevails. A primer for systematic reviews and meta-analysis in the sports science clearly articulates a clear and direct purpose delineation process for syntheses (Impellizzeri and Bizzini 2021). Goals checklist List from (Impellizzeri and Bizzini 2021): Identifying treatments that are not effective. Summarizing the likely magnitude of benefits of effective treatments. Identifying unanticipated risks of apparently effective treatments. Identifying gaps of knowledge. Auditing the quality of existing trials. Collectively, these how-to papers suggest that it would be ideal if synthesis reporting exceeded the norms and standards associated with primary research reporting to enable next-level synthesis and reproducibility. Nonetheless, it is easy to get lost in the technical details. Consequently, purpose, audience, and reuse should be kept at the forefront of reviewing, reporting, and doing scientific syntheses. These ideas can be mobilized for knowledge mining even in the early steps of evidence retrieval and reviewing for inclusion in a synthesis project. Challenge Apply the checklist to a systematic review and meta-analysis in your discipline published a number of years ago and again to a more recent synthesis study. Check whether the derived data were also published for each synthesis paper. Check main text of each and supplements and note whether a Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) checklist or flow diagram with provided. Do a primary publication research query using Google Scholar and/or The Web Of Science for your specific process. Document the relative frequency of reporting by search terms (complete template provided below). Review a small subset of the papers (screen only) and test out the PRISMAstatement R package for this scoping review. Products A checklist for review and reporting of these specific syntheses. A pilot literature dataset of the literature for a search(es) for a synthesis. 3. A PRISMA statement flow chart for use in retrospective pilot reporting. Resources Slide deck A synthesis review literature template PRISMA statement arguments for R package Reflection questions Was there evidence for a change in better synthesis reporting practices in your discipline? Were the derived data reported in syntheses or would it be possible to update/repeat a published synthesis in your discipline? Does the flow chart to reviewing and reusing research align with your cognitive modality and critical thinking approach to evidence? "],["meta.html", "Chapter 4 Meta-analysis in R", " Chapter 4 Meta-analysis in R Learning outcomes Format data for meta-analyses in R. Explore the capacity of the R package metafor. Pilot statistics for two datasets. Context Try out these ideas in a meta-analysis. Securing derived data to replicate an existing synthesis was not common historically, but it is now becoming increasingly viable with open science influences on these contributions. How to do a meta-analysis is well described in the peer-reviewed literature (Field and Gillett 2010) and numerous books (Koricheva et al 2013) to name a few. Doing meta-analyses using the R programming language is similarly well articulated - particularly from the documentation associated with the package metafor. There are no bad choices in R with well over 100 packages associated with and supporting meta-analyses (conservatively listed at 151 packages. The output of Stata (an application specific to many medical meta-analyses), the R package meta, and metafor were virtually identical in several test cases (Lortie and Filazzola 2020). Ten criteria are proposed in contrasting R packages for this task specifically, but at the current time, metafor is the most commonly used and extensively documented. Hence, consider tackling the challenges here with this package as a robust starting point and entry in meta-statistics. The 5 primary steps for meta-analyses in R. Secure primary data. Build conceputal model for factors, reponses, and moderators. Calculate effect size(s). Fit appropriate meta-model. Explore significance levels, heterogeneity, and bias. Challenge Start simple and go with a classic. These wind turbine data and its meta-analysis changed the world. Download these data and appled the 5 steps from above with help from the metafor documentation. In the spirit of reuse and conceputal replication, this process was reported in 2017 with a more comprehensive dataset. Try out one of these datasets. Preventing exercise-induced bronchoconstriction meta-analysis A meta-analysis of restoration outcomes and species richness A synthesis of common garden studies testing for local adaptation to climate A synthesis shrub facilitation studies testing for increases in community diversity estimates Products Two R scripts for meta-analysis. A sense of data structures needed for meta-analyses in the R package metafor. Resources metafor documentation Bookdown free books Reflection questions Did the analytical process differ significantly from primary-data workflows? Do meta-models in R sufficiently report outcomes to assess strength of evidence? What other steps would be an appropriate addendum to this process? What relational or qualitative elements might be worthwhile to consider adding to meta-analyses and their interpretation for stronger evidence framing and reuse? "],["interpretation.html", "Chapter 5 Quantitative synthesis workflow reporting", " Chapter 5 Quantitative synthesis workflow reporting Learning outcomes Interpret meta-analyses. Leverage meta-analyses to inform decision making. Appreciate extent of synthesis tools capacities to direct inference. Context Interpreting meta-analyses and systematic reviews effectively and fairly is critical for the advancement of scientific theory. Conceptual and methodological developments in many fields currently rely on syntheses to evaluate the relative merit of contrasting options (Halpern et al 2020). Importantly however, meta-analyses can fail (Kotiaho and Tomkins 2002). The representativeness of the evidence is crucial because the intent and purpose of the primary studies should in principle align with the purpose of the synthesis (as discussed previously). The selection process for evidence and the statistics can introduce biases and lead to potential spurious interpretations of the relevance of a hypothesis for instance. Failure to find support for a hypothesis does not necessarily mean that this is not a conceptually valid endeavor or that it does not explain the functioning of a specific system. Sparse evidence or publication biases can skew negative results in meta-analyses and lead to erroneous interpretations because of the evidence. What we know about the world (or the functioning of the world) may not match what we know about the science about the world. However, support for hypotheses in spite of biases and evidence limitations is likely to be representative of the underlying processes and patterns in a system. Consequently, meta-analyses can be evaluated, at times based on evidence volume, as more one-sided versus two-sided tests of concepts or methods. Heterogeneity and moderator analyses will also temper the assessment capacity for a meta-analysis to succeed in functioning as a knowledge engineering tool. Evaluation and use of meta-analyses is relevant to society at large. Controversy can be resolved or obfuscated by these syntheses (De Vrieze 2018). Two key factors play into the public reuse arena. The interpretation process and the relative strength of the derived evidence between syntheses that diverge in their conclusions. The collision of ideas can be exacerbated when synthesist scientists do not transparently and clearly report findings. This can be further magnified when the media and others directly reuse a published meta-analysis. The other factor, strength of evidence, must be transparently and accurately handled within each meta-analysis by better reporting. Lack of supporting derived data and inadequate reporting of the study populations (of papers) that comprise contrasting meta-analyses impede quantitative contrasts of syntheses. Description of the differences between studies within a synthesis is foundational to a better mapping of science onto ‘truth’ (Ioannidis 2005). We must set a higher standard for synthesis reporting and adopt more open science components into these projects. Finally, education and discussion within a synthesis of how well that specific process functioned in summarizing an evidence hierarchy must be developed. Challenge Select a meta-analysis or systematic review with extensive reporting. Assess whether the interpretations are well supported by the evidence that was incorporated in the synthesis. Explore one of the case studies provided in this short course and examine the sensitivity of the conclusions to reductions in the volume of evidence. Test full versus reduced models in one of the examples, statistically, to explore moderator influences on net outcomes. Products A set of evidence from a meta-analysis with a sense of study quality. A simple script to explore the statistical interpretations of meta-analyses done in R. Resources Slide deck ROSES RepOrting standards for Systematic Evidence Syntheses: pro forma template Reflection questions Do primary studies need to be scored for quality? Do single large primary studies inform meta-analyses more than many smaller trials or experiments? Is there a mechanism qualitatively or quantitatively to demonstrate representativeness and address matching truths to a system? "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
